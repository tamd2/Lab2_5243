Tim Taylor and Taumer Anabtawi
Cse 5243
Lab 3
Naive Bayes and K Nearest Neighbors algorithms

How to run these algorithms:
    Included in this submission are two README files; one for each of the 
algorithms implemented. Please refer to these READMEs for instruction on how
to run these.

Who did what:
    Tim created the naive bayes algorithm and all graphs shown in the report

    Taumer created the KNN algorithm and 

Assumptions made in this project:
    Naive Bayes:
        Each misclassification within a topic label feature vector contributes
        to the number of true positives/false positives/etc. So if the topic
        label feature vector has 5 words, a predicted classification may 
        give up to 5 false positives, or 5 true positives, and so on.

        All body text feature vectors are treated as presence vectors. So
        one feature vector with some word "X" valued at 4 contributes
        to the probability of "X" the same as a feature vector with "X" valued
        at 1. This was done to simplify what the probability of a given word is
        for computing priors.

        If a classification is equally likely to be classified as 1 as it is 0,
        It is classified at 0. This was because the feature vectors were very 
        sparse so 0 should be the default case.

    KNN: 


Scalability of building model:
    Scalability of building Naive Bayes:
The jump from 500 vectors to 1000 vectors conferred a jump in time to train 
that took 4 times as long. This suggests a n^2 complexity (for n initial
vectors) for training. Jumping from 1000 documents up to 2000 took roughly 4 
times as long, so this also suggests a n^2 complexity. This is moderately
acceptable for scalability purposes. You can see this growth in the graph below

    <graph for documents trained vs time to build model>


    Scalability of building KNN:


Scalability of classifying unknowns:
    Scalability of Classifying unknowns with Naive Bayes:
The jump from 500 documents to 1000 documents led to avg classify time to
jump by a factor of 3 (~5 to ~15) and the jump from 1000 to 2000 led to
a similar jump by a factor of about 3 (~15 to ~45). These two facts would
point to the growth of the time to classify being a n^(3/2) complexity

The complexity for training is higher than the complexity for classifying
unknowns, so the overall program scales alongside the training complexity
rather than the testing complexity.
 
An important factor that is obscured in this is how the number of unique
words in the feature vectors affect this run time. When introducing more
documents, you likely add more unique words which in turn increases the
size of the feature vectors that are processed.

You can see the growth of time taken to classify unknowns in the graph below

    <graph for documents trained vs avg time to classify an unknown>


    Scalability of Classifying unknowns with KNN:


Effect of number of documents trained on:
    Effect of number of documents trained on Naive Bayes:

    Effect of number of documents trained on KNN:



Accuracy of Classifiers:
    Accuracy of Naive Bayes:
An important factor in this project was how sparse the data was in the
topic label feature vectors. There were often more than 100 times the
amount of 0's than 1's in the topic label feature vectors. With this in
mind, a model could achieve fairly high accuracy by simply predicting all
topic labels as 0. To work around that, I want to focus mostly on how well
the model led to true positives. That being said, I will still discuss
overall accuracy.

    Accuracy:
    As you can see by the graphs below, accuracy does not change much regardless of what percent of documents you train on. 

    <two graphs for accuracy>

    However, this does not tell the full story of the data. In order to get a better picture, you have to look at the precision,
    recall, and F-measure. This is important due to the class imbalance within the data due to very sparse topic label feature 
    vectors.

    Precision:
    As you can see by the graphs below, precision does seem more affected by the percent of documents trained on compared to
    accuracy. For the case of 500 documents processed, precision increases when the percent of documents trained on increases.
    This is a pretty intuitive result. However, the case for 1000 documents shows a much more sporadic picture. There doesn't
    seem to be a trend in this graph, and that is likely due to the underlying documents being ordered in a way that makes
    learning difficult. See the note below the F-Measure section for further discussion on this.

    <two graphs for precision>

    Recall:
    As you can see by the graphs below, recall behaves very similar to precision. In the 500 document case, it increases as the
    percent of documents trained on increases. But again, the trend is lost in the 1000 document case.

    <two graphs for recall>

    F-Measure:
    Finally, as you can see below, f-measure behaves similarly to both precision and recall. It tends to increase as percent
    of documents trained on increases. The 1000 document case messes this up a bit though.
    
    <two graphs for f-measure>

    Note: The documents and how they are ordered affects the results of this
    naive bayes classifier very much. As such, some runs were more successful
    than others possibly due in part to which documents it got to train on
    as opposed to how many documents it got to train on.
 
    Listed below are three runs worth noting that shed light on this:

<make a table out of the below>
% Trained   Num of Docs           accuracy 
90          500          ...      99.0 
90          600          ...      71.2  
90          700          ...      99.0  
 
    This effect is most notable in comparing the two runs of 90% trained, 500
    and 600 documents trained on. The accuracy for 500 documents was 99% but
    the accuracy for 600 documents was 71%. This significant drop is most
    likely due to unfamiliar behaving documents being introduced around the
    500th document mark. I feel confident saying this because running the
    classifier with the same parameters but on 700 documents erased this
    significant drop in accuracy, brining it back up to 99%. This hints that
    the model wasn't being tested on documents that were very dissimilar from
    its training documents



    Accuracy of KNN:



