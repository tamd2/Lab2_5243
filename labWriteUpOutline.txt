Tim Taylor and Taumer Anabtawi
cse5243, Lab 2

What is implemented:
    for this lab, we have implemented K-Means and dbscan for our clustering algorithms. For our two distance metrics, we chose to
implement Manhattan distance and Euclidian distance.

    our projects run from input files that are generated from the previous lab. As such, the run time for our project does not depend
at all on the run time of the previous lab.


Underlying assumptions:
    for K-Means:

    for dbscan:
        1) If a point is equal to epsilon in distance away, it is counted as a neighbor
        2) A point cannot be its own neighbor
       
    for measuring clustering quality:
        1) for entropy, I used log base 2 as this class recommends
 

How to use this project:
    There are two python script files for this project, one for K-Means, and one for dbscan. Each has a README file that will
explain how to run the file including how to change the parameters for K, epsilon, input data, etc.


Scalability:
    for K-Means:


    for dbscan:
        You can see below for a graph that shows how fast dbscan ran for a given amount of files to process. When the amount of files
    to process doubled (from 5,000 to 10,000) the run time increased by a factor of 4 (5 minutes to 20 minutes). Similarly, when the 
    amount of files to process tripled (from 5,000 to 15,000) the runtime took about 9 times longer (5 minutes to ~45 minutes). This
    suggests that we have a polynomial scale for runtime of n^2. This scale is decent. It isn't great, but it exponential which makes
    it managable.
        
    <include pictures for documents vs time>

    <discuss if manhattan vs euclidian affected time taken>

Quality of clustering:
    for K-Means:


    for dbscan:

